{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-Baseline-Code",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wqei_pHop-VtyOkNTz8YXT2PnA81gOf_",
      "authorship_tag": "ABX9TyMs+aR/v3ryiZcRaR/etrEN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walwi878/LSTM-Baseline-Code-Sentiment-Analysis/blob/main/LSTM_Baseline_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn-_iimrJ2y8"
      },
      "source": [
        "#William Wallace\n",
        "#17/03/21\n",
        "#Assignment 1 AIML428 (NLP and Text Mining)\n",
        "#This CNN was build with a lot of help from the following two tutorials: \n",
        "#https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
        "#https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "# LSTM and CNN for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "# Random seed is fixed for repeatability \n",
        "numpy.random.seed(80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lT5uWNoHByC"
      },
      "source": [
        "# Keeps the top n words for the loaded built-in dataset, and zeros the remaining words\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlwG1smvHF7Y"
      },
      "source": [
        "# Truncates and pads input sequences to ensure vectors are the same size\n",
        "max_review_len = 600\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_len)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_len)\n",
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS9ErfvVHK2Y",
        "outputId": "543043de-566c-4441-8e7e-64e4d61ae12c"
      },
      "source": [
        "# Defines the model's architecture\n",
        "embedding_vector_len = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_len, input_length=max_review_len))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 600, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 600, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 300, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 216,405\n",
            "Trainable params: 216,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "196/196 [==============================] - 141s 710ms/step - loss: 0.6371 - accuracy: 0.5958\n",
            "Epoch 2/3\n",
            " 17/196 [=>............................] - ETA: 2:06 - loss: 0.2918 - accuracy: 0.8860"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_rR2EpIHPTn"
      },
      "source": [
        "# Results and accuracy\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}